---
title: "Bulk Uploading Mammal Data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Bulk Uploading Mammal Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The purpose of the `bulkUploader` package is to help data managers to bring their data into the [Neotoma Paleoecology Database](https://neotomadb.org).  Neotoma is a highly structured database that contains information on a large number of data types, including diatoms, ostracodes, pollen and mammal fossils.

Neotoma is really a "database of databases", and contains X constituent databases.  To help data managers include their data in Neotoma, we have wrapped Neotoma package functions around tools to help manage, validate and upload their data into Neotoma.

```{r setup, results='hide', message=FALSE}
devtools::install_github('NeotomaDB/bulkuploader')
library(bulkUploader)
devtools::install_github('NeotomaDB/neotoma2')
library(neotoma2)
library(purrr)
library(dplyr)
```

## Managing your Data

The easiest data to enter into Neotoma is the publication data and contact data.  From a given database, or data resource, we want to check that information is accurate.  For contacts, unless we have ORCIDs for all users, we're stuck hand-checking information.  For publication records we can use the DOI as a "gold standard" for publication information, since CrossRef manages this data for us, and provides a unique identifier that we can check against Neotoma's information.

### Publication Data

However your publication data is organized, it must be aligned with Neotoma's so that it can be included in the database.  The Neotoma `publications` table uses the following fields in the `publication` object:

| Field | Description |
=========== | =================
| publicationtype | The type of publication |
| publicationid | A unique Neotoma publication identifier |
| articletitle | The title of the document |
| year | The publishing year where available |
| journal | The journal (if available) |
| volume | Journal volume |
| issue | Journal issue |
| pages | Pages over which the item spans |
| citation | A plain text citation, generally APA format. |
| doi | The article DOI |
| author |  A list of `contacts` |

Users can directly add new information into Neotoma using the `neotoma2::add_publication()` function, or they can use `scrape_dois()`, a function in the `bulkUploader` package, to validate publication information and to add additional metadata, where such metadata does not already exist.

If a user wants to use `scrape_dois()`, a function in the `bulkUploader` package, they need to first match  their dataframe column names with the appropriate column names:

First, we load in the "convertdata" function
```{r convertdatafcn, results='hide'}
convertdata <- function(x, author, year, title, journal, booktitle){
    # note: I think this should handle a case where booktitle is missing, but all others are required
  
    if (missingArg(booktitle)){
      x <- x %>%
        select(author = as.character(author), 
               year = as.character(year), 
               title = as.character(title), 
               journal = as.character(journal))
    }else{
      x <- x %>%
        select(author = as.character(author), 
               year = as.character(year), 
               title = as.character(title), 
               journal = as.character(journal),
               booktitle = as.character(booktitle))
    }
}
```

Next, import user data and convert to the bulkuploader format
```{r importdata, results='hide'}

originalcsv <- miomapcsv #point to the appropriate R object for the imported dataframe
# Note that `miomapcsv` is a data element contained within the `bulkUploader` package, but that this could be any dataframe that the user has imported

# do you want to only run for a subset of rows (10) to test functionality? change between y or n
testing <- 'y'
testrows <- 10

dat_arranged <- convertdata(miomapcsv, "Author", "Year", "Title", "Journal", "Book.Title")
```


```{r scrapedois, results='hide'}

# Setup the variables for storing output ahead of time.
results <- data.frame(score=rep(NA, nrow(dat_arranged)), delta = NA)
outputs <- list()

# Ideally this code would be massaged.  The issue here is that there is title and journal information in several places.  It's not the nicest way of doing things.

if(file.exists("outputs.RDS")) {
  outputs <- readRDS("outputs.RDS")
}

# specify the number of rows to run
if (testing == 'y'){
  rowsToRun <- testrows
  }else{
    rowsToRun <- nrow(dat_arranged)
  }

for (i in (length(outputs) + 1):rowsToRun) {

  # JLB note sure why Simon added this 'if' statement
  # Aren't ALL rows returned as TRUE because that's how the 'results' dataframe is specified?
  # Maybe this makes sure that the results file was actually created?
  if (is.na(results$score[i])) { 

    # Testing to see if there's a journal or book title.
    container = ifelse(is.na(dat_arranged[i, 'journal']),
                       dat_arranged[i, 'booktitle'],
                       dat_arranged[i, 'journal'])
    
    if (is.na(container)) {
      message("Row ", i, " is missing a journal or book title.")
    } else {
      outputs[[i]] <- scrape_dois(title = dat_arranged[i, 'title'],
                          year = dat_arranged[i, 'year'])
      
      if (length(outputs[[i]]) > 0) {
        
        results$score[i] <- outputs[[i]][[1]]$score
        
        if (length(outputs[[i]]) > 1) {
          results$delta[i] <- outputs[[i]][[1]]$score - outputs[[i]][[2]]$score
        }
        
        outputs[[i]]$results <- results[i,]
        
        message("Tested article ", i)
        
      } else {
        # The result row will have an NA for that row.
        message("Couldn't get a match for ", i)
      }
    }
  }
  saveRDS(outputs, "output.RDS")
}

```

From this output we have a `list` object called `outputs`, with a length equal to the number of publications in the original data file.  We also have a `data.frame` called `results` with columns `score` and `delta`.

The `score` gives the approximate match strength to a publication in CrossRef.  The `delta` lets us know what the score of the next closest match is.  Because we need to work with the CrossRef API in a reasonable way, this matching tends to be slow.

Once this script runs we then have a set of publications for the constituent database, and for each described publication we have 0 or more potential matches.  To generate a simple match we want to create some form of citation.  We can do this using the very simple helper function `makeCite()`.

```{r reviewPubs}

arrangedcites <- makeCite(dat_arranged$author,
                     dat_arranged$year,
                     dat_arranged$title,
                     dat_arranged$journal)

crossCites <- outputs %>% map(cr_to_neotoma)

```

Now, for each publication we have a set of possible matches, and scores.  We need to prompt the user to select the best match:

```{r getMatches, results='hide'}
bestMatch <- list()

for(i in 1:length(crossCites)) {
  bestMatch[[i]] <- findMatch(arrangedcites[i], results[i,], crossCites[[i]])
  if (!('publication' %in% class(bestMatch[[i]]) | is.na(bestMatch[[i]]))) {
    bestMatch[[i]] <- NULL
    break
  }
}

```
